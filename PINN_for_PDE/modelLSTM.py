import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import hyperpar as hp
import os
#Device seting
device = torch.device("cuda" if torch.cuda .is_available() else "cpu")

"""This is a simple implementation of a Physics Informed Neural Network (PINN) for solving the 1D Burgers' equation.
It is based on a feedforward Multilayer Perceptron (MLP) architecture. The PINN is trained to minimize the residual of the PDE, as well as the boundary and initial conditions."""
#PINN class

#============================================================= PINN with LSTM =========================================================
#LSTM PINN class
class LSTM_PINN(nn.Module):
    def __init__(self, Nx, seq_len, hidden_size, num_layers):
        super().__init__()
        self.seq_len = seq_len
        self.Nx = Nx
        self.lstm = nn.LSTM(input_size = Nx, hidden_size = hidden_size, num_layers = num_layers, batch_first = True)
        self.fc = nn.Linear(hidden_size, Nx)
        
        #initialize the weights and biases of the LSTM and the fully connected layer
        for name, param in self.lstm.named_parameters():
            if "weight" in name:
                nn.init.xavier_normal_(param)
            elif "bias" in name:
                nn.init.zeros_(param)
                
        nn.init.xavier_normal_(self.fc.weight)
        nn.init.zeros_(self.fc.bias)
    
    def forward(self, seq_u):
        #Here, seq_u is a tensor of shape [batch, seq_len, Nx]
        #It is a sequence of length seq_len, where each element is a tensor of shape [batch, Nx]
        out, _ = self.lstm(seq_u)
        last = out[:, -1, :]
        return self.fc(last) #output is a tensor of shape [batch, Nx]
    
def generate_sequence_data(N_samples, seq_len, Nx, dt):
    """ Create random sequences of length seq_length+1 from the true solution
    (e.g. a dataset precomputed or generated by a high-fidelity solver)."""
    
    x = np.linspace(0,1,Nx)
    t0 = np.linspace(0, hp.T_max-seq_len*dt, N_samples) #random initial times
    # t0 = np.random.rand(N_samples)*(hp.T_max-seq_len*dt)
    seqs = np.zeros((N_samples, seq_len+1, Nx), float)
    
    for i in range(N_samples):
        for k in range(seq_len+1):
            ti = t0[i]+k*dt
            #example exact Burgers sol: sin(pi x)*exp(-pi^2 nu t)
            seqs[i, k, :] = np.sin(np.pi*x)*np.exp(-np.pi**2*hp.NU*ti)
    
    #split into input seq and target next step
    U_seq = seqs[:, :seq_len, :].astype(np.float32)
    U_next = seqs[:, seq_len, :].astype(np.float32)
    return(
        torch.tensor(U_seq, device = device), 
        torch.tensor(U_next, device = device)
    )
    

def trainLSTM():
    """Train the LSTM PINN"""
    print("Training the LSTM PINN...")
    history = {"total_loss": []}
    
    #Hyperparameters from hyperpar.py
    Nx, seq_len = hp.NX, hp.SEQ_LEN
    hidden_size = hp.HIDDEN_SIZE
    num_layers = hp.NUM_LAYERS
    lr = hp.LR
    epochs = hp.EPOCHS
    N_samples = hp.N_SAMPLES
    dt = hp.DT
    
    #Model, optimizer, loss
    model = LSTM_PINN(Nx, seq_len, hidden_size, num_layers).to(device)
    optimizer = optim.Adam(model.parameters(), lr = lr)
    mse = nn.MSELoss()
    
    #prepare training data 
    U_seq, U_next = generate_sequence_data(N_samples, seq_len, Nx, dt)
    
    #training loop
    for epoch in range(epochs):
        optimizer.zero_grad()
        pred = model(U_seq) #predictions of the model
        loss = mse(pred, U_next)
        loss.backward()
        optimizer.step()
        history["total_loss"].append(loss.item())
        
        if epoch % 100 == 0:
            print(f"Epoch {epoch}, Loss: {loss.item():.4e}")
    
    #write the model to a file
    if (hp.SAVE_MODEL):
        try:
            os.remove("modelLSTM.pth")
        except OSError:
            pass
        torch.save(model.state_dict(), "modelLSTM.pth")
        print("Model saved to modelLSTM.pth")
    
    #Save the losses
    if (hp.SAVE_LOSS):
        try:
            os.remove("lossLSTM.npy")
        except OSError:
            pass
        np.save("lossLSTM.npy", history)
        print("Losses saved to lossLSTM.npy")

if __name__ == "__main__":
    trainLSTM()