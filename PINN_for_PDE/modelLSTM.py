import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import hyperpar as hp
import os
#Device seting
device = torch.device("cuda" if torch.cuda .is_available() else "cpu")

"""This is a simple implementation of a Physics Informed Neural Network (PINN) for solving the 1D Burgers' equation.
It is based on a feedforward Multilayer Perceptron (MLP) architecture. The PINN is trained to minimize the residual of the PDE, as well as the boundary and initial conditions."""
#PINN class

#============================================================= PINN with LSTM =========================================================
#LSTM PINN class
class LSTM_PINN(nn.Module):
    def __init__(self, Nx, seq_len, hidden_size, num_layers):
        super().__init__()
        self.seq_len = seq_len
        self.Nx = Nx
        self.lstm = nn.LSTM(input_size = Nx, hidden_size = hidden_size, num_layers = num_layers, batch_first = True)
        self.fc = nn.Linear(hidden_size, Nx)
        
        #initialize the weights and biases of the LSTM and the fully connected layer
        for name, param in self.lstm.named_parameters():
            if "weight" in name:
                nn.init.xavier_normal_(param)
            elif "bias" in name:
                nn.init.zeros_(param)
                
        nn.init.xavier_normal_(self.fc.weight)
        nn.init.zeros_(self.fc.bias)
    
    def forward(self, seq_u):
        #Here, seq_u is a tensor of shape [batch, seq_len, Nx]
        #It is a sequence of length seq_len, where each element is a tensor of shape [batch, Nx]
        out, _ = self.lstm(seq_u)
        last = out[:, -1, :]
        return self.fc(last) #output is a tensor of shape [batch, Nx]
    
def generate_sequence_data(N_samples, seq_len, Nx, dt):
    """ Create random sequences of length seq_length+1 from the true solution
    (e.g. a dataset precomputed or generated by a high-fidelity solver)."""
    
    x = np.linspace(0,1,Nx)
    # t0 = np.linspace(0.01, hp.T_max, N_samples) #random initial times
    t0 = np.random.rand(N_samples)*(hp.T_max)
    seqs = np.zeros((N_samples, seq_len+1, Nx), float)
    
    for i in range(N_samples):
        for k in range(seq_len+1):
            ti = t0[i]+k*dt
            #example exact Burgers sol: sin(pi x)*exp(-pi^2 nu t)
            seqs[i, k, :] = np.sin(np.pi*x)*np.exp(-np.pi**2*hp.NU*ti)
    
    #split into input seq and target next step
    U_seq = seqs[:, :seq_len, :].astype(np.float32)
    U_next = seqs[:, seq_len, :].astype(np.float32)
    return(
        torch.tensor(U_seq, device = device), 
        torch.tensor(U_next, device = device)
    )

def generate_sequence_data_from_explicit_solver(
    N_samples: int,
    seq_len:   int,
    Nx:        int,
    dt:        float,
    nu:        float
):
    """
    Explicit forward‐Euler solver for 1D Burgers on [0,1], Dirichlet u=0 @ x=0,1.
    Returns U_seq of shape [N_samples, seq_len, Nx] and U_next [N_samples, Nx].
    """
    x  = np.linspace(0, 1, Nx, dtype=np.float32)
    dx = x[1] - x[0]

    # Stability check
    dt_max = 0.5 * min(dx, dx*dx/nu)
    if dt > dt_max:
        raise ValueError(f"dt={dt:.3e} is too large for stability; must ≤ {dt_max:.3e}")

    seqs = np.zeros((N_samples, seq_len+1, Nx), dtype=np.float32)

    for s in range(N_samples):
        # initial condition
        u = np.sin(np.pi*x).astype(np.float32)
        seqs[s,0,:] = u

        for n in range(seq_len):
            u_new = np.zeros_like(u)

            # interior points 1..Nx-2
            u_new[1:-1] = (
                 u[1:-1]
               - dt * u[1:-1] * ( u[2:]   - u[:-2]  )/(2*dx)
               + nu * dt * ( u[2:]   - 2*u[1:-1] + u[:-2] )/(dx*dx)
            )

            # Dirichlet BC
            u_new[0]  = 0.0
            u_new[-1] = 0.0

            seqs[s, n+1, :] = u_new
            u = u_new

    U_seq  = seqs[:, :seq_len, :].copy()
    U_next = seqs[:,  seq_len, :].copy()

    # to torch
    return (
        torch.tensor(U_seq,  device=device, dtype=torch.float32),
        torch.tensor(U_next,device=device, dtype=torch.float32),
    )

    
# #Calcoliamo i residui per equazione di Burgers: u_t + u u_x - nu u_xx = 0
# def pde_res(model, x, t, nu):
#     """Compute the PDE residual with automatic differentiation. Here some informations about network
#     ♪ create_graph = True allows to compute the gradient of the residual w.r.t. the input
#     ♪
#     ♪
#     ♪"""
#     x.requires_grad_(True)
#     t.requires_grad_(True)
#     u = model(x, t)
#     u_t = torch.autograd.grad(u, t, grad_outputs = torch.ones_like(u), create_graph = True)[0]
#     u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]
#     u_xx = torch.autograd.grad(u_x, x, grad_outputs = torch.ones_like(u_x), create_graph = True)[0]

#     return u_t-u*u_x+nu*u_xx

def trainLSTM():
    """Train the LSTM PINN"""
    print("Training the LSTM PINN...")
    history = {"total_loss": []}
    
    #Hyperparameters from hyperpar.py
    Nx, seq_len = hp.NX, hp.SEQ_LEN
    hidden_size = hp.HIDDEN_SIZE
    num_layers = hp.NUM_LAYERS
    lr = hp.LR
    epochs = hp.EPOCHS
    N_samples = hp.N_SAMPLES
    dt = hp.DT
    
    #Model, optimizer, loss
    model = LSTM_PINN(Nx, seq_len, hidden_size, num_layers).to(device)
    optimizer = optim.Adam(model.parameters(), lr = lr)
    mse = nn.MSELoss()
    
    #prepare training data 
    U_seq, U_next = generate_sequence_data(N_samples, seq_len, Nx, dt)
    # U_seq, U_next = generate_sequence_data_from_explicit_solver(N_samples, seq_len, Nx, dt, hp.NU)
    
    #training loop
    for epoch in range(epochs):
        optimizer.zero_grad()
        pred = model(U_seq) #predictions of the model
        loss = mse(pred, U_next)
        loss.backward()
        optimizer.step()
        history["total_loss"].append(loss.item())
        
        if epoch % 100 == 0:
            print(f"Epoch {epoch}, Loss: {loss.item():.4e}")
    
    #write the model to a file
    if (hp.SAVE_MODEL):
        try:
            os.remove("modelLSTM.pth")
        except OSError:
            pass
        torch.save(model.state_dict(), "modelLSTM.pth")
        print("Model saved to modelLSTM.pth")
    
    #Save the losses
    if (hp.SAVE_LOSS):
        try:
            os.remove("lossLSTM.npy")
        except OSError:
            pass
        np.save("lossLSTM.npy", history)
        print("Losses saved to lossLSTM.npy")

def trainLSTM_with_Physics():
    """Train the LSTM PINN with PDE, BC and IC losses."""
    print("Training the LSTM PINN with physics losses…")
    history = {
        "total_loss": [], 
        "data_loss": [], 
        "pde_loss": [], 
        "bc_loss": [], 
        "ic_loss": []
    }
    Nx, seq_len = hp.NX, hp.SEQ_LEN
    hidden_size = hp.HIDDEN_SIZE
    num_layers = hp.NUM_LAYERS
    lr, epochs = hp.LR, hp.EPOCHS
    N_samples, dt = hp.N_SAMPLES, hp.DT
    nu = hp.NU
    #weighting factors for the loss components
    lambda_data, lambda_pde = hp.LAMBDA_DATA, hp.LAMBDA_PDE
    lambda_bc, lambda_ic = hp.LAMBDA_BC, hp.LAMBDA_IC
    
    #model, optimizer, loss
    model = LSTM_PINN(Nx, seq_len, hidden_size, num_layers).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    mse = nn.MSELoss()
    
    #Data: sequences and “true” next‐step
    # U_seq, U_next = generate_sequence_data(N_samples, seq_len, Nx, dt)
    U_seq, U_next = generate_sequence_data_from_explicit_solver(N_samples, seq_len, Nx, dt, nu)
    
    #Pre-compute BC IC ground truth on the grid
    x_grd =np.linspace(0, 1, Nx)
    x_bc_= [0, Nx-1] #left/right boundary indices
    x_ic_ = np.sin(np.pi*x_grd) #u(x,0)
    
    for ep in range(epochs):
        optimizer.zero_grad()
        # 1) Data‐loss
        pred = model(U_seq)
        loss_data = mse(pred, U_next)
        
        u_n = U_seq[:, -1, :]
        u_t_approx = (pred - u_n) / dt
        # 1) first prediction:  u^{n+1}
        pred_n1 = model(U_seq)            # shape [batch, Nx]

        # 2) roll the window forward one step: drop u^{n-seq_len+1}, append pred_n1
        U_seq2  = torch.cat([
            U_seq[:, 1:, :],             # old: all but the first in the sequence
            pred_n1.unsqueeze(1)         # new: the freshly predicted u^{n+1}
        ], dim=1)                         # now shape [batch, seq_len, Nx]

        # 3) second prediction:  u^{n+2}
        pred_n2 = model(U_seq2)           # shape [batch, Nx]

        # 4) approximate time‐derivative at t_{n+1}
        u_t_approx = (pred_n2 - pred_n1) / dt
         
        dx = 1.0/(Nx-1)
        # u_x = (u_n[:, 2:] - u_n[:, :-2])/(dx) #interior
        # u_xx = (u_n[:, 2:]-2*u_n[:, 1:-1]+u_n[:, :-2])/(dx*dx) #interior second derivative
        # u_x = torch.cat([torch.zeros(u_n.shape[0], 1, device = device), u_x, torch.zeros(u_n.shape[0], 1, device = device)], dim = 1)
        # u_xx = torch.cat([torch.zeros(u_n.shape[0], 1, device = device), u_xx, torch.zeros(u_n.shape[0], 1, device = device)], dim = 1)
        #explicit solver for pred
        u_x = (pred[:, 2:] - pred[:, :-2])/(2*dx) #interior
        u_xx = (pred[:, 2:] - 2*pred[:, 1:-1] + pred[:, :-2])/(dx*dx)
        u_x = torch.cat([torch.zeros(pred.shape[0], 1, device = device), u_x, torch.zeros(pred.shape[0], 1, device = device)], dim = 1)
        u_xx = torch.cat([torch.zeros(pred.shape[0], 1, device = device), u_xx, torch.zeros(pred.shape[0], 1, device = device)], dim = 1)
        # u_xx = torch.cat([torch.zeros(pred.shape[0], 1, device = device), u_xx, torch.zeros(pred.shape[0], 1, device = device)], dim = 1)
        #Burgers residual
        res = u_t_approx -u_n*u_x + nu*u_xx
        loss_pde = mse(res, torch.zeros_like(res))
        
        #3) BC loss (u(0,t) = u(1,t) = 0)
        loss_bc = mse(pred[:, x_bc_], torch.zeros(pred.shape[0], 2, device = device))
        
        #4) IC loss (u(x,0) = u_ic(x))
        ic_target = torch.tensor(x_ic_, device = device).unsqueeze(0).repeat(pred.shape[0], 1)
        loss_ic = mse(U_seq[:, 0, :], ic_target)
        
        #5) Total loss
        loss = (lambda_data*loss_data + lambda_pde*loss_pde + lambda_bc*loss_bc +lambda_ic*loss_ic)
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), 1.0) #gradient clipping
        optimizer.step()
         #6) save the losses
        history["total_loss"].append(loss.item())
        history["data_loss"].append(loss_data.item())
        history["pde_loss"].append(loss_pde.item())
        history["bc_loss"].append(loss_bc.item())
        history["ic_loss"].append(loss_ic.item())
        
        if ep % 10 ==0:
            print(f"Epoch {ep}, Loss: {loss.item():.4e}, Data loss: {loss_data.item():.4e}, PDE loss: {loss_pde.item():.4e}, BC loss: {loss_bc.item():.4e}, IC loss: {loss_ic.item():.4e}") 
        
    #7) save things
    if hp.SAVE_MODEL:
        try:
            os.remove("modelLSTM.pth")
        except OSError:
            pass
        torch.save(model.state_dict(), "modelLSTM.pth")
        print("Model saved to modelLSTM.pth")
        
    if hp.SAVE_LOSS:
        try:
            os.remove("lossLSTM.npy")
        except OSError:
            pass
        np.save("lossLSTM.npy", history)
        print("Losses saved to lossLSTM.npy")
if __name__ == "__main__":
    trainLSTM_with_Physics()